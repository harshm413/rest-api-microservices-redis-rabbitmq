# ðŸš€ Real-World Kubernetes Deployment Guide

## Microservices Production Deployment Simulation with K3d

This guide documents our complete journey to simulate real-world enterprise Kubernetes deployment locally using K3d, following industry best practices.

---

## ðŸŽ¯ **Project Overview**

**Goal:** Deploy REST API Microservices application using real-world enterprise patterns:

- **Stateless services** in Kubernetes (like AWS EKS)
- **Stateful services** as external managed services (like AWS RDS, ElastiCache)
- **Multi-environment setup** (Dev, Staging, Production)
- **Rolling updates** and zero-downtime deployments
- **Real-world networking** and service discovery

---

## ðŸ“‹ **Phase 1: Infrastructure Setup**

### **Step 1: Install K3d & kubectl**

```bash
# Install K3d (Lightweight Kubernetes)
curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash

# Install kubectl
curl -LO "https://dl.k8s.io/release/v1.28.0/bin/windows/amd64/kubectl.exe"
move kubectl.exe C:\Windows\System32\

# Verify installations
k3d version
kubectl version --client
```

### **Step 2: Create K3d Cluster (3-Node Setup)**

```bash
# Create cluster with 1 control-plane + 2 workers (simulating real EKS)
k3d cluster create chatapp --servers 1 --agents 2 --port "8080:80@loadbalancer"
```

**âœ… Verification:**

```bash
# Check cluster nodes
kubectl get nodes

# Expected output:
NAME                   STATUS   ROLES                  AGE   VERSION
k3d-chatapp-agent-0    Ready    <none>                 40s   v1.31.5+k3s1
k3d-chatapp-agent-1    Ready    <none>                 39s   v1.31.5+k3s1
k3d-chatapp-server-0   Ready    control-plane,master   46s   v1.31.5+k3s1

# Check cluster info
kubectl cluster-info

# Expected output:
Kubernetes control plane is running at https://host.docker.internal:6776
CoreDNS is running at https://host.docker.internal:6776/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
Metrics-server is running at https://host.docker.internal:6776/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy
```

### **Step 3: Create Environment Namespaces**

```bash
# Create environment namespaces (simulating AWS accounts)
kubectl create namespace dev
kubectl create namespace staging
kubectl create namespace prod

# Verify namespaces
kubectl get namespaces
```

**âœ… Verification:**

```bash
# Check namespaces
kubectl get namespaces

# Expected output:
NAME              STATUS   AGE
default           Active   3m50s
dev               Active   22s
kube-node-lease   Active   3m50s
kube-public       Active   3m50s
kube-system       Active   3m50s
prod              Active   12s
staging           Active   16s
```

---

## ðŸ—„ï¸ **Phase 2: External Services Setup (Simulating AWS Managed Services)**

### **Architecture Decision:**

- **Kubernetes:** Only for stateless microservices
- **Docker:** For stateful services (simulating AWS RDS, ElastiCache, Amazon MQ)

### **Step 4: Setup External Services for All Environments**

```bash
# Create external services network
docker network create microservices-external

# DEV Environment Services (ports 33xx)
docker run -d --name dev-mysql --network microservices-external -p 3301:3306 \
  -e MYSQL_ROOT_PASSWORD=dev_pass -e MYSQL_DATABASE=auth_service \
  -e MYSQL_USER=auth_user -e MYSQL_PASSWORD=auth_pass mysql:8.0

docker run -d --name dev-postgres --network microservices-external -p 5431:5432 \
  -e POSTGRES_DB=user_service -e POSTGRES_USER=user_user \
  -e POSTGRES_PASSWORD=user_pass postgres:16

docker run -d --name dev-mongo --network microservices-external -p 27016:27017 \
  -e MONGO_INITDB_ROOT_USERNAME=root -e MONGO_INITDB_ROOT_PASSWORD=dev_pass mongo:7

docker run -d --name dev-redis --network microservices-external -p 6378:6379 redis:7

docker run -d --name dev-rabbitmq --network microservices-external \
  -p 5671:5672 -p 15671:15672 rabbitmq:3-management

# STAGING Environment Services (ports 34xx)
docker run -d --name staging-mysql --network microservices-external -p 3302:3306 \
  -e MYSQL_ROOT_PASSWORD=staging_pass -e MYSQL_DATABASE=auth_service \
  -e MYSQL_USER=auth_user -e MYSQL_PASSWORD=auth_pass mysql:8.0

docker run -d --name staging-postgres --network microservices-external -p 5432:5432 \
  -e POSTGRES_DB=user_service -e POSTGRES_USER=user_user \
  -e POSTGRES_PASSWORD=user_pass postgres:16

docker run -d --name staging-mongo --network microservices-external -p 27017:27017 \
  -e MONGO_INITDB_ROOT_USERNAME=root -e MONGO_INITDB_ROOT_PASSWORD=staging_pass mongo:7

docker run -d --name staging-redis --network microservices-external -p 6379:6379 redis:7

docker run -d --name staging-rabbitmq --network microservices-external \
  -p 5672:5672 -p 15672:15672 rabbitmq:3-management

# PROD Environment Services (ports 35xx)
docker run -d --name prod-mysql --network microservices-external -p 3303:3306 \
  -e MYSQL_ROOT_PASSWORD=prod_pass -e MYSQL_DATABASE=auth_service \
  -e MYSQL_USER=auth_user -e MYSQL_PASSWORD=auth_pass mysql:8.0

docker run -d --name prod-postgres --network microservices-external -p 5433:5432 \
  -e POSTGRES_DB=user_service -e POSTGRES_USER=user_user \
  -e POSTGRES_PASSWORD=user_pass postgres:16

docker run -d --name prod-mongo --network microservices-external -p 27018:27017 \
  -e MONGO_INITDB_ROOT_USERNAME=root -e MONGO_INITDB_ROOT_PASSWORD=prod_pass mongo:7

docker run -d --name prod-redis --network microservices-external -p 6380:6379 redis:7

docker run -d --name prod-rabbitmq --network microservices-external \
  -p 5673:5672 -p 15673:15672 rabbitmq:3-management
```

### **Environment Port Mapping:**

| Service     | Dev Port | Staging Port | Prod Port | Simulates          |
| ----------- | -------- | ------------ | --------- | ------------------ |
| MySQL       | 3301     | 3302         | 3303      | AWS RDS MySQL      |
| PostgreSQL  | 5431     | 5432         | 5433      | AWS RDS PostgreSQL |
| MongoDB     | 27016    | 27017        | 27018     | AWS DocumentDB     |
| Redis       | 6378     | 6379         | 6380      | AWS ElastiCache    |
| RabbitMQ    | 5671     | 5672         | 5673      | AWS Amazon MQ      |
| RabbitMQ UI | 15671    | 15672        | 15673     | Management Console |

---

## ðŸ—ï¸ **Phase 3: Docker Images & Kubernetes Manifests**

### **Step 5: Build Docker Images**

**âš ï¸ Critical Fix Required:** The Dockerfiles need workspace linking fixes for pnpm monorepos.

```bash
# Build all service images for dev environment
docker build -f services/auth-service/Dockerfile -t microservices/auth:dev .
docker build -f services/user-service/Dockerfile -t microservices/user:dev .
docker build -f services/chat-service/Dockerfile -t microservices/chat:dev .
docker build -f services/gateway-service/Dockerfile -t microservices/gateway:dev .

# Import images into k3d cluster
k3d image import microservices/auth:dev microservices/user:dev microservices/chat:dev microservices/gateway:dev -c chatapp

# Verify images
docker images | grep microservices
```

**âœ… Expected Output:**
```bash
microservices/gateway   dev    abc123   2 minutes ago   150MB
microservices/auth      dev    def456   3 minutes ago   145MB
microservices/user      dev    ghi789   4 minutes ago   140MB
microservices/chat      dev    jkl012   5 minutes ago   155MB
```

**ðŸ”§ Docker Build Issues & Solutions:**

**Problem 1: pnpm workspace dependencies not found**
```bash
# Error: Cannot find module 'zod' or its corresponding type declarations
```

**Solution:** Copy ALL workspace package.json files before `pnpm install`:
```dockerfile
# âŒ Wrong - Only copying some packages
COPY packages/common/package.json ./packages/common/
COPY services/auth-service/package.json ./services/auth-service/

# âœ… Correct - Copy ALL workspace packages
COPY packages/common/package.json ./packages/common/
COPY services/auth-service/package.json ./services/auth-service/
COPY services/user-service/package.json ./services/user-service/
COPY services/chat-service/package.json ./services/chat-service/
COPY services/gateway-service/package.json ./services/gateway-service/
```

**Problem 2: Workspace linking broken after copying source**
```bash
# Error: Dependencies missing during build
```

**Solution:** Re-run `pnpm install` after copying source code:
```dockerfile
FROM deps AS builder
# Copy source code
COPY packages/common ./packages/common
COPY services/auth-service ./services/auth-service

# âœ… Critical: Reinstall to ensure proper workspace linking
RUN pnpm install --frozen-lockfile

# Now build will work
RUN cd /app && pnpm --filter @rest-api/common build
RUN cd /app && pnpm --filter @rest-api/auth-service build
```

### **Step 6: Create Environment-Specific ConfigMaps (Non-Sensitive Only)**

```bash
# Create k8s directory structure
mkdir -p k8s/dev k8s/staging k8s/prod

# Create dev configmap YAML (non-sensitive config only)
cat > k8s/dev/configmap.yaml << EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: dev-config
  namespace: dev
data:
  NODE_ENV: "development"
  GATEWAY_PORT: "4000"
  AUTH_SERVICE_PORT: "4003"
  USER_SERVICE_PORT: "4001"
  CHAT_SERVICE_PORT: "4002"
  JWT_EXPIRES_IN: "1d"
  JWT_REFRESH_EXPIRES_IN: "30d"
  SERVICE_NAME: "microservices-dev"
EOF

# Create staging configmap YAML
cat > k8s/staging/configmap.yaml << EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: staging-config
  namespace: staging
data:
  NODE_ENV: "staging"
  GATEWAY_PORT: "4000"
  AUTH_SERVICE_PORT: "4003"
  USER_SERVICE_PORT: "4001"
  CHAT_SERVICE_PORT: "4002"
  JWT_EXPIRES_IN: "1d"
  JWT_REFRESH_EXPIRES_IN: "30d"
  SERVICE_NAME: "microservices-staging"
EOF

# Create prod configmap YAML
cat > k8s/prod/configmap.yaml << EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: prod-config
  namespace: prod
data:
  NODE_ENV: "production"
  GATEWAY_PORT: "4000"
  AUTH_SERVICE_PORT: "4003"
  USER_SERVICE_PORT: "4001"
  CHAT_SERVICE_PORT: "4002"
  JWT_EXPIRES_IN: "1d"
  JWT_REFRESH_EXPIRES_IN: "30d"
  SERVICE_NAME: "microservices-prod"
EOF
```

### **Step 7: Create Secrets (All Sensitive Data)**

```bash
# Create dev secrets YAML (ALL sensitive data)
cat > k8s/dev/secrets.yaml << EOF
apiVersion: v1
kind: Secret
metadata:
  name: dev-secrets
  namespace: dev
type: Opaque
stringData:
  JWT_SECRET: "dev_jwt_secret_key_here_32_chars_min"
  JWT_REFRESH_SECRET: "dev_refresh_secret_key_here_32_chars"
  INTERNAL_API_TOKEN: "dev_internal_api_token_here_secure"
  AUTH_DB_URL: "mysql://auth_user:auth_pass@host.docker.internal:3301/auth_service"
  USER_DB_URL: "postgres://user_user:user_pass@host.docker.internal:5431/user_service"
  MONGO_URL: "mongodb://root:dev_pass@host.docker.internal:27016/chat_service?authSource=admin"
  REDIS_URL: "redis://host.docker.internal:6378"
  RABBITMQ_URL: "amqp://guest:guest@host.docker.internal:5671"
EOF

# Create staging secrets YAML
cat > k8s/staging/secrets.yaml << EOF
apiVersion: v1
kind: Secret
metadata:
  name: staging-secrets
  namespace: staging
type: Opaque
stringData:
  JWT_SECRET: "staging_jwt_secret_key_here_32_chars_min"
  JWT_REFRESH_SECRET: "staging_refresh_secret_key_here_32_chars"
  INTERNAL_API_TOKEN: "staging_internal_api_token_here_secure"
  AUTH_DB_URL: "mysql://auth_user:auth_pass@host.docker.internal:3302/auth_service"
  USER_DB_URL: "postgres://user_user:user_pass@host.docker.internal:5432/user_service"
  MONGO_URL: "mongodb://root:staging_pass@host.docker.internal:27017/chat_service?authSource=admin"
  REDIS_URL: "redis://host.docker.internal:6379"
  RABBITMQ_URL: "amqp://guest:guest@host.docker.internal:5672"
EOF

# Create prod secrets YAML
cat > k8s/prod/secrets.yaml << EOF
apiVersion: v1
kind: Secret
metadata:
  name: prod-secrets
  namespace: prod
type: Opaque
stringData:
  JWT_SECRET: "prod_jwt_secret_key_here_32_chars_min_SECURE"
  JWT_REFRESH_SECRET: "prod_refresh_secret_key_here_32_chars_SECURE"
  INTERNAL_API_TOKEN: "prod_internal_api_token_here_VERY_SECURE"
  AUTH_DB_URL: "mysql://auth_user:auth_pass@host.docker.internal:3303/auth_service"
  USER_DB_URL: "postgres://user_user:user_pass@host.docker.internal:5433/user_service"
  MONGO_URL: "mongodb://root:prod_pass@host.docker.internal:27018/chat_service?authSource=admin"
  REDIS_URL: "redis://host.docker.internal:6380"
  RABBITMQ_URL: "amqp://guest:guest@host.docker.internal:5673"
EOF
```

**ðŸ”’ Security Note:**
- **ConfigMaps:** Non-sensitive configuration only (ports, environment names)
- **Secrets:** ALL sensitive data (passwords, tokens, connection URLs)
- **YAML Files:** Version controlled, reviewable, GitOps compatible

### **Step 8: Apply Configuration**

```bash
# Apply dev environment configuration
kubectl apply -f k8s/dev/configmap.yaml
kubectl apply -f k8s/dev/secrets.yaml

# Apply staging environment configuration
kubectl apply -f k8s/staging/configmap.yaml
kubectl apply -f k8s/staging/secrets.yaml

# Apply prod environment configuration
kubectl apply -f k8s/prod/configmap.yaml
kubectl apply -f k8s/prod/secrets.yaml

# Verify configuration
kubectl get configmaps -n dev
kubectl get secrets -n dev
```

### **Step 9: Create Kubernetes Deployment Manifests**

```bash
# Create auth service deployment YAML
cat > k8s/dev/auth-deployment.yaml << EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: auth-service
  namespace: dev
  labels:
    app: auth-service
    tier: backend
spec:
  replicas: 1
  selector:
    matchLabels:
      app: auth-service
  template:
    metadata:
      labels:
        app: auth-service
        tier: backend
    spec:
      containers:
      - name: auth-service
        image: microservices/auth:dev
        ports:
        - containerPort: 4003
        envFrom:
        - configMapRef:
            name: dev-config
        - secretRef:
            name: dev-secrets
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
        livenessProbe:
          httpGet:
            path: /health
            port: 4003
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 4003
          initialDelaySeconds: 5
          periodSeconds: 5
EOF
```

**ðŸ­ Enterprise Patterns Used:**
- **YAML Manifests:** Version controlled, reviewable
- **Resource Limits:** Prevent resource starvation
- **Health Probes:** Automatic restart and traffic routing
- **Labels:** Proper service identification
- **Separate ConfigMaps/Secrets:** Security best practices

---

## ðŸš€ **Phase 4: Deploy to Dev Environment**

### **Step 9: Apply Configuration & Deploy Services**

```bash
# Apply dev environment configuration
kubectl apply -f k8s/dev/configmap.yaml -f k8s/dev/secrets.yaml

# Deploy all services
kubectl apply -f k8s/dev/auth-deployment.yaml -f k8s/dev/auth-service.yaml
kubectl apply -f k8s/dev/user-deployment.yaml -f k8s/dev/user-service.yaml
kubectl apply -f k8s/dev/chat-deployment.yaml -f k8s/dev/chat-service.yaml
kubectl apply -f k8s/dev/gateway-deployment.yaml -f k8s/dev/gateway-service.yaml
```

### **âš ï¸ Critical Kubernetes Deployment Fix**

**Problem:** Deployments using wrong image pattern
```yaml
# âŒ Wrong - Building code inside container (anti-pattern)
containers:
- name: auth-service
  image: node:22-alpine
  command: ["sh", "-c", "npm install -g pnpm && cd /app && pnpm install..."]
```

**Solution:** Use pre-built Docker images
```yaml
# âœ… Correct - Use immutable pre-built images
containers:
- name: auth-service
  image: microservices/auth:dev
  imagePullPolicy: IfNotPresent
  ports:
  - containerPort: 4003
```

### **Step 10: Monitor Deployment Status**

```bash
# Check all pods status
kubectl get pods -n dev

# Expected healthy state:
# NAME                              READY   STATUS    RESTARTS   AGE
# auth-service-xxx                  1/1     Running   0          2m
# user-service-xxx                  1/1     Running   0          2m
# gateway-service-xxx               1/1     Running   0          2m
# chat-service-xxx                  1/1     Running   0          2m
```

### **ðŸ” Troubleshooting Common Issues**

**Issue 1: CrashLoopBackOff - Database Connection**
```bash
# Check logs for connection errors
kubectl logs -n dev deploy/auth-service --tail=10

# Common error: ECONNREFUSED to database
# Solution: Ensure external services are running
docker ps --filter name=dev-mysql --filter name=dev-postgres
```

**Issue 2: MongoDB Authentication Failed**
```bash
# Error in logs: "Authentication failed", code: 18
# Root cause: MongoDB container missing root user

# Fix: Recreate MongoDB with proper initialization
docker stop dev-mongo && docker rm dev-mongo
docker run -d --name dev-mongo --network microservices-external -p 27016:27017 \
  -e MONGO_INITDB_ROOT_USERNAME=root -e MONGO_INITDB_ROOT_PASSWORD=dev_pass mongo:7

# Restart chat service
kubectl delete pod -n dev -l app=chat-service
```

**Issue 3: ImagePullBackOff**
```bash
# Error: Failed to pull image
# Solution: Import images into k3d cluster
k3d image import microservices/auth:dev -c chatapp
```

---

## ðŸ§ª **Phase 5: Testing & Validation**

### **Step 13: Test Dev Environment**

```bash
# Check all pods are running
kubectl get pods -n dev

# Test API endpoints
curl http://localhost:8080/health
curl -X POST http://localhost:8080/auth/register -H "Content-Type: application/json" -d '{"email":"test@dev.com","displayName":"Dev User","password":"DevPass123!"}'

# Check logs
kubectl logs -n dev -l app=gateway-service
kubectl logs -n dev -l app=auth-service
```

### **Step 14: Monitor Resources**

```bash
# Check resource usage
kubectl top pods -n dev
kubectl describe hpa -n dev

# Check service endpoints
kubectl get endpoints -n dev
```

---

## ðŸ”„ **Phase 6: Rolling Updates & Operations**

### **Step 15: Simulate Rolling Update**

```bash
# Update image version
kubectl set image deployment/auth-service -n dev auth-service=microservices/auth:v1.1

# Watch rolling update
kubectl rollout status deployment/auth-service -n dev

# Check rollout history
kubectl rollout history deployment/auth-service -n dev
```

### **Step 16: Rollback Simulation**

```bash
# Rollback to previous version
kubectl rollout undo deployment/auth-service -n dev

# Verify rollback
kubectl rollout status deployment/auth-service -n dev
```

### **Step 17: Scale Services**

```bash
# Manual scaling
kubectl scale deployment auth-service --replicas=3 -n dev

# Check HPA scaling
kubectl get hpa -n dev --watch
```

---

## ðŸŒ **Phase 7: Multi-Environment Deployment**

### **Step 18: Deploy to Staging**

```bash
# Build staging images
docker build -f services/gateway-service/Dockerfile -t microservices/gateway:staging .

# Deploy to staging namespace
kubectl apply -f k8s/staging/ -n staging

# Test staging environment
curl http://localhost:8081/health
```

### **Step 19: Deploy to Production**

```bash
# Build production images
docker build -f services/gateway-service/Dockerfile -t microservices/gateway:prod .

# Deploy to production namespace
kubectl apply -f k8s/prod/ -n prod

# Test production environment
curl http://localhost:8082/health
```

---

## ðŸ“Š **Phase 8: Monitoring & Observability**

### **Step 20: Setup Monitoring**

```bash
# Install Prometheus & Grafana (optional)
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm install prometheus prometheus-community/kube-prometheus-stack -n monitoring --create-namespace

# Access Grafana dashboard
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80
```

### **Step 21: Log Aggregation**

```bash
# View aggregated logs
kubectl logs -n dev -l tier=backend --tail=100

# Stream logs from all services
kubectl logs -n dev -f -l app=gateway-service
```

---

## ðŸ“Š **Current Status**

### **âœ… Completed:**

- [x] K3d cluster setup (3 nodes)
- [x] Environment namespaces (dev, staging, prod)
- [x] External services deployment (MySQL, PostgreSQL, MongoDB, Redis, RabbitMQ)
- [x] Docker images built with pnpm workspace fixes
- [x] Kubernetes manifests created with proper image references
- [x] Dev environment deployed successfully
- [x] Production-grade troubleshooting completed

### **ðŸ† Deployment Results:**

```bash
# Final pod status in dev namespace:
NAME                              READY   STATUS    RESTARTS   AGE
auth-service-xxx                  1/1     Running   10         15m
user-service-xxx                  1/1     Running   10         12m
gateway-service-xxx               1/1     Running   1          12m
chat-service-xxx                  1/1     Running   0          2m
```

**âœ… Success Metrics:**
- **4/4 services** running successfully
- **Gateway accessible** at http://localhost:8080
- **Database connections** established
- **Zero-downtime deployment** achieved
- **Production-ready patterns** implemented

### **ðŸ”§ Key Issues Resolved:**

1. **pnpm workspace dependencies** - Fixed by copying all package.json files
2. **Docker build failures** - Fixed by re-linking workspace after source copy
3. **Kubernetes anti-patterns** - Fixed by using pre-built images instead of runtime builds
4. **MongoDB authentication** - Fixed by proper container initialization
5. **Image availability** - Fixed by importing images into k3d cluster

### **ðŸ“š Lessons Learned:**

- **Kubernetes only runs immutable images** - Never build code at runtime
- **pnpm workspaces require all package.json files** for proper dependency resolution
- **External services need proper initialization** - Especially authentication setup
- **Production debugging requires systematic log analysis** - Not guesswork
- **Real-world deployments rarely work perfectly first time** - Troubleshooting is key

---

## ðŸŽ¯ **Real-World Simulation Benefits**

This setup perfectly simulates:

- **AWS EKS** (K3d cluster)
- **AWS RDS** (External MySQL/PostgreSQL)
- **AWS DocumentDB** (External MongoDB)
- **AWS ElastiCache** (External Redis)
- **AWS Amazon MQ** (External RabbitMQ)
- **Multi-Account Strategy** (Separate namespaces)
- **Enterprise DevOps Practices** (Environment promotion)

---

## ðŸ“š **Learning Outcomes**

By completing this guide, you'll understand:

- Real-world Kubernetes deployment patterns
- Stateless vs Stateful service architecture
- Environment separation and promotion
- Service discovery and networking
- Rolling updates and zero-downtime deployments
- Production-ready configuration management

---

**Next:** Continue with Phase 2 - External Services Deployment! ðŸš€
